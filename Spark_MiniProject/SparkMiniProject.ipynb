{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "#### COMMON FILE PROCESSING FUNCTION ####\r\n",
        "def process_file(file):\r\n",
        "    print(\"Processing File: \" + file)\r\n",
        "    make, year = None, None\r\n",
        "    raw = spark.sparkContext.textFile(file)\r\n",
        "    vin_kv = raw.map(lambda x: extract_vin_key_value(x))\r\n",
        "    enhance_make = vin_kv.groupByKey().flatMap(lambda kv: populate_make(kv[1]))\r\n",
        "    make_kv = enhance_make.map(lambda x: extract_make_key_value(x))\r\n",
        "    final_results = make_kv.reduceByKey(lambda x, y: x + y).collect()\r\n",
        "    for result in final_results:\r\n",
        "        print(str(result[0]) + \",\" + str(result[1]))\r\n",
        "\r\n",
        "def process_file(file):\r\n",
        "    \"\"\"Process a file by first propagating vehicle make and model year to all accident records (incident_type='A')\r\n",
        "    based off of VIN number from initial sales (incident_type='I') and then sum all accident records grouping by \r\n",
        "    vehicle make and model year\"\"\"\r\n",
        "    print(\"Processing File: \" + file)\r\n",
        "    raw = spark.sparkContext.textFile(file)\r\n",
        "    vin_kv = raw.map(lambda x: extract_vin_key_value(x))\r\n",
        "    enhance_make = vin_kv.groupByKey().flatMap(lambda kv: populate_make(kv[1]))\r\n",
        "    make_kv = enhance_make.map(lambda x: extract_make_key_value(x))\r\n",
        "    final_results = make_kv.reduceByKey(lambda x, y: x + y).collect()\r\n",
        "    for result in final_results:\r\n",
        "        print(str(result[0]) + \",\" + str(result[1]))\r\n",
        "\r\n",
        "def extract_vin_key_value(line):\r\n",
        "    \"\"\"Create a key / value tuple with VIN and vehicle info\"\"\"\r\n",
        "    line = line.strip()\r\n",
        "    line = line.split(\",\")\r\n",
        "    vin = line[2]\r\n",
        "    vals = (line[2],line[1],line[3],line[5]) #(vin, inc_type, make, year)\r\n",
        "    return (vin, vals)\r\n",
        "\r\n",
        "def populate_make(values):\r\n",
        "    \"\"\"Propagate vehicle info to all accident records (incident_type='A)\"\"\"\r\n",
        "    output = []\r\n",
        "    for mkyr in values:\r\n",
        "        if mkyr[1]=='I':\r\n",
        "            make = mkyr[2]\r\n",
        "            year = mkyr[3]\r\n",
        "    for acc in values:\r\n",
        "        if acc[1] == 'A':\r\n",
        "            output.append((acc[0], acc[1], make, year))\r\n",
        "    return output\r\n",
        "\r\n",
        "def extract_make_key_value(x):\r\n",
        "    \"\"\"Create Key/Value tuples with Make-Year and Count of 1\"\"\"\r\n",
        "    return (x[2] + \"-\" + x[3], 1)\r\n",
        "\r\n",
        "#### CREATE SPARK SESSION ####       \r\n",
        "spark = SparkSession.builder.master('local').appName('My Application').getOrCreate()\r\n",
        "spark.conf.set(\r\n",
        "    \"fs.azure.account.key.azblobstoragedwilde.blob.core.windows.net\",\r\n",
        "    secrets.get('azblobkey')\r\n",
        "    )\r\n",
        "\r\n",
        "#### PROCESS CSV FILE IN THE SparkMiniProject DIRECTORY ####\r\n",
        "process_file(\"wasbs://mycontainer@azblobstoragedwilde.blob.core.windows.net/SparkMiniProject/data.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "dwildesparkpool",
              "session_id": 16,
              "statement_id": 53,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-07T05:30:21.5440316Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-07T05:30:21.9365491Z",
              "execution_finish_time": "2022-01-07T05:30:23.7423687Z"
            },
            "text/plain": "StatementMeta(dwildesparkpool, 16, 53, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing File: wasbs://mycontainer@azblobstoragedwilde.blob.core.windows.net/SparkMiniProject/data.csv\nNissan-2003,1\nMercedes-2016,1\nMercedes-2015,2"
          ]
        }
      ],
      "execution_count": 53,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": " Spark Mini Project: Post-Sales Report",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}