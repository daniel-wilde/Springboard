{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\r\n",
        "from datetime import datetime\r\n",
        "from decimal import Decimal\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.types import DateType, DecimalType, IntegerType, StructType, StructField, StringType, TimestampType\r\n",
        "from notebookutils import mssparkutils\r\n",
        "\r\n",
        "#### Create Final Output Schema ####\r\n",
        "final_schema = StructType([\r\n",
        "    StructField('trade_dt', DateType(), True),\r\n",
        "    StructField('rec_type', StringType(), True),\r\n",
        "    StructField('symbol', StringType(), True),\r\n",
        "    StructField('exchange', StringType(), True),\r\n",
        "    StructField('event_tm', TimestampType(), True),\r\n",
        "    StructField('event_seq_nb', IntegerType(), True),\r\n",
        "    StructField('arrival_tm', TimestampType(), True),\r\n",
        "    StructField('trade_pr', DecimalType(30,15), True),\r\n",
        "    StructField('trade_size', IntegerType(), True),\r\n",
        "    StructField('bid_pr', DecimalType(30,15), True),\r\n",
        "    StructField('bid_size', IntegerType(), True),\r\n",
        "    StructField('ask_pr', DecimalType(30,15), True),\r\n",
        "    StructField('ask_size', IntegerType(), True),\r\n",
        "    StructField('partition', StringType(), True),\r\n",
        "  ])\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "dwildesparkpool",
              "session_id": 10,
              "statement_id": 30,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-12-21T05:50:19.7730132Z",
              "session_start_time": null,
              "execution_start_time": "2021-12-21T05:50:19.9049202Z",
              "execution_finish_time": "2021-12-21T05:50:20.0503671Z"
            },
            "text/plain": "StatementMeta(dwildesparkpool, 10, 30, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#### COMMON FUNCTION FOR RETURNING DATA IN SAME FORMAT ####\r\n",
        "def common_event(\r\n",
        "    trade_dt: DateType, \r\n",
        "    rec_type: StringType, \r\n",
        "    symbol: StringType, \r\n",
        "    exchange: StringType, \r\n",
        "    event_tm: TimestampType,\r\n",
        "    event_seq_nb: IntegerType,\r\n",
        "    arrival_tm: TimestampType,\r\n",
        "    trade_pr: DecimalType(30,15),\r\n",
        "    trade_size: IntegerType,\r\n",
        "    bid_pr: DecimalType(30,15),\r\n",
        "    bid_size: IntegerType,\r\n",
        "    ask_pr: DecimalType(30,15),\r\n",
        "    ask_size: IntegerType,\r\n",
        "    partition: StringType,\r\n",
        "    line: StringType):\r\n",
        "    \r\n",
        "    if partition == \"B\":\r\n",
        "        return line     #### put bad lines into their own file\r\n",
        "    else:\r\n",
        "        return [trade_dt, rec_type, symbol, exchange, \r\n",
        "                event_tm, event_seq_nb, arrival_tm, \r\n",
        "                trade_pr, trade_size, bid_pr, bid_size, ask_pr, ask_size, partition]"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "dwildesparkpool",
              "session_id": 10,
              "statement_id": 31,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-12-21T05:50:19.8207469Z",
              "session_start_time": null,
              "execution_start_time": "2021-12-21T05:50:20.1300701Z",
              "execution_finish_time": "2021-12-21T05:50:20.2791762Z"
            },
            "text/plain": "StatementMeta(dwildesparkpool, 10, 31, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### FUNCTION FOR PARSING ALL .CSV FILES ####\r\n",
        "def parse_csv(line:str):\r\n",
        "    record_type_pos = 2\r\n",
        "    record = line.split(\",\")\r\n",
        "    try:\r\n",
        "        if record[record_type_pos] == \"T\":\r\n",
        "            event = common_event(datetime.strptime(record[0], \"%Y-%m-%d\"), record[2], record[3], record[6], \r\n",
        "                datetime.strptime(record[4], '%Y-%m-%d %H:%M:%S.%f'), int(record[5]), datetime.strptime(record[1], '%Y-%m-%d %H:%M:%S.%f'), \r\n",
        "                Decimal(record[7]), int(record[8]), None, None, None, None, \"T\", None)\r\n",
        "            return event\r\n",
        "        elif record[record_type_pos] == \"Q\":\r\n",
        "            event = common_event(datetime.strptime(record[0], \"%Y-%m-%d\"), record[2], record[3], record[6], \r\n",
        "                datetime.strptime(record[4], '%Y-%m-%d %H:%M:%S.%f'), int(record[5]), datetime.strptime(record[1], '%Y-%m-%d %H:%M:%S.%f'), \r\n",
        "                None, None, Decimal(record[7]), int(record[8]), Decimal(record[9]), int(record[10]), \"Q\", None)\r\n",
        "            return event\r\n",
        "    except Exception as e:\r\n",
        "        return common_event(None, None, None, None, None, None, None, None, None, None, None, None, None, \"B\", line)  \r\n",
        "\r\n",
        "#### FUNCTION FOR PARSING ALL .JSON FILES ####\r\n",
        "def parse_json(line:str):\r\n",
        "    record = json.loads(line)\r\n",
        "    record_type = record[\"event_type\"]\r\n",
        "    try:\r\n",
        "        if record_type == \"T\":\r\n",
        "            event = common_event(datetime.strptime(record[\"trade_dt\"], \"%Y-%m-%d\"), record[\"event_type\"], record[\"symbol\"], record[\"exchange\"], \r\n",
        "                datetime.strptime(record[\"event_tm\"], '%Y-%m-%d %H:%M:%S.%f'), int(record[\"event_seq_nb\"]), datetime.strptime(record[\"file_tm\"], '%Y-%m-%d %H:%M:%S.%f'), \r\n",
        "                Decimal(record[\"price\"]), int(record[\"size\"]), None, None, None, None, \"T\", None)\r\n",
        "            return event\r\n",
        "        elif record_type == \"Q\":\r\n",
        "            event = common_event(datetime.strptime(record[\"trade_dt\"], \"%Y-%m-%d\"), record[\"event_type\"], record[\"symbol\"], record[\"exchange\"], \r\n",
        "                datetime.strptime(record[\"event_tm\"], '%Y-%m-%d %H:%M:%S.%f'), int(record[\"event_seq_nb\"]), datetime.strptime(record[\"file_tm\"], '%Y-%m-%d %H:%M:%S.%f'), \r\n",
        "                None, None, Decimal(record[\"bid_pr\"]), int(record[\"bid_size\"]), Decimal(record[\"ask_pr\"]), int(record[\"ask_size\"]), \"Q\", None)\r\n",
        "            return event\r\n",
        "    except Exception as e:\r\n",
        "        return common_event(None, None, None, None, None, None, None, None, None, None, None, None, None, \"B\", line) "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "dwildesparkpool",
              "session_id": 10,
              "statement_id": 32,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-12-21T05:50:19.8790028Z",
              "session_start_time": null,
              "execution_start_time": "2021-12-21T05:50:20.3817807Z",
              "execution_finish_time": "2021-12-21T05:50:20.5268059Z"
            },
            "text/plain": "StatementMeta(dwildesparkpool, 10, 32, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### RECURSE AZURE BLOB STORAGE PATH AND LIST ALL FILES ####\r\n",
        "def get_files_recursive(path: str, folder_depth: int):\r\n",
        "    all_files = mssparkutils.fs.ls(path)\r\n",
        "    for f in all_files:\r\n",
        "        if f.size != 0:\r\n",
        "            yield f\r\n",
        "    if folder_depth > 1:\r\n",
        "        for f in all_files:\r\n",
        "            if f.size != 0:\r\n",
        "                continue\r\n",
        "            for p in get_files_recursive(f.path, folder_depth - 1):\r\n",
        "                yield p\r\n",
        "    else:\r\n",
        "        for f in all_files:\r\n",
        "            if f.size == 0:\r\n",
        "                yield f\r\n",
        "\r\n",
        "#### COMMON FILE PROCESSING FUNCTION ####\r\n",
        "def process_files(azure_container_path, parser_func):\r\n",
        "    all_files = get_files_recursive(azure_container_path, folder_depth=10)\r\n",
        "    for file in all_files:\r\n",
        "        if file.path.endswith(\".txt\"):\r\n",
        "            print(\"Processing File: \" + file.path)\r\n",
        "            raw = spark.sparkContext.textFile(file.path)\r\n",
        "            parsed = raw.map(lambda line: parser_func(line))\r\n",
        "            data = spark.createDataFrame(parsed, schema=final_schema)\r\n",
        "            data.write.partitionBy(\"partition\").mode(\"append\").parquet(\"output_dir\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "dwildesparkpool",
              "session_id": 10,
              "statement_id": 33,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-12-21T05:50:19.9477375Z",
              "session_start_time": null,
              "execution_start_time": "2021-12-21T05:50:20.629187Z",
              "execution_finish_time": "2021-12-21T05:50:20.7890958Z"
            },
            "text/plain": "StatementMeta(dwildesparkpool, 10, 33, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### CREATE SPARK SESSION ####       \r\n",
        "spark = SparkSession.builder.master('local').appName('app').getOrCreate()\r\n",
        "spark.conf.set(\r\n",
        "    \"fs.azure.account.key.azblobstoragedwilde.blob.core.windows.net\",\r\n",
        "    \"0ZxdhT9CBrrHGmBKo6tJ+YNEI6XSX94hSqwcN6NfuNm1gzGof8BQmfgGppQ84IfA+9GFFTntSJ5owSXDsDDNyw==\"\r\n",
        "    )\r\n",
        "\r\n",
        "#### PROCESS ALL CSV FILES IN THE data/csv DIRECTORY ####\r\n",
        "process_files(\"wasbs://mycontainer@azblobstoragedwilde.blob.core.windows.net/data/csv\", parse_csv)\r\n",
        "\r\n",
        "#### PROCESS ALL CSV FILES IN THE data/json DIRECTORY ####\r\n",
        "process_files(\"wasbs://mycontainer@azblobstoragedwilde.blob.core.windows.net/data/json\", parse_json)\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "dwildesparkpool",
              "session_id": 10,
              "statement_id": 34,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-12-21T05:50:20.0432412Z",
              "session_start_time": null,
              "execution_start_time": "2021-12-21T05:50:20.8720238Z",
              "execution_finish_time": "2021-12-21T05:50:27.6136414Z"
            },
            "text/plain": "StatementMeta(dwildesparkpool, 10, 34, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing File: wasbs://mycontainer@azblobstoragedwilde.blob.core.windows.net/data/csv/2020-08-05/NYSE/part-00000-5e4ced0a-66e2-442a-b020-347d0df4df8f-c000.txt\nProcessing File: wasbs://mycontainer@azblobstoragedwilde.blob.core.windows.net/data/csv/2020-08-06/NYSE/part-00000-214fff0a-f408-466c-bb15-095cd8b648dc-c000.txt\nProcessing File: wasbs://mycontainer@azblobstoragedwilde.blob.core.windows.net/data/json/2020-08-06/NASDAQ/part-00000-092ec1db-39ab-4079-9580-f7c7b516a283-c000.txt\nProcessing File: wasbs://mycontainer@azblobstoragedwilde.blob.core.windows.net/data/json/2020-08-05/NASDAQ/part-00000-c6c48831-3d45-4887-ba5f-82060885fc6c-c000.txt"
          ]
        }
      ],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}